---
title: Agentica > Guide Documents > Core Library > Prompt Histories
---
import { Tabs } from "nextra/components";

import AgenticaPromptSnippet from "../../snippets/AgenticaPromptSnippet.mdx";
import AgenticaOperationSelectionSnippet from "../../snippets/AgenticaOperationSelectionSnippet.mdx";
import AgenticaOperationSnippet from "../../snippets/AgenticaOperationSnippet.mdx";

## Prompt Histories
<Tabs items={[
  <code>src/main.ts</code>,
  <code>AgenticaPrompt</code>,
  <code>IAgenticaPromptJson</code>,
]}>
  <Tabs.Tab>
```typescript filename="src/main.ts" showLineNumbers {16, 21}
import {
  Agentica,
  AgenticaPrompt,
  IAgenticaPromptJson,
} from "@agentica/core";
import OpenAI from "openai";

export const main = async (id: string): Promise<void> => {
  const agent = new Agentica({
    model: "chatgpt",
    vendor: {
      api: new OpenAI({ apiKey: "*****" }),
      model: "gpt-4o-mini",
    },
    controllers: [...],
    histories: (await getHistories(id)) satisfies IAgenticaPromptJson[],
  });
  const prompts: AgenticaPrompt[] = await agent.conversate(
    "I wanna buy MacBook Pro",
  );
  await archiveHistories(id, prompts.map((p) => p.toJSON()));
};
```
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaPromptSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSelectionSnippet />
  </Tabs.Tab>
</Tabs>

You can start chatbot by creating `Agentica` instance, and calling its function `Agentica.conversate()`. When creating the `Agentica` instance, you have to specify the LLM (Large Language Model) service vendor, and lists of controller to serving for function calling.

In the `Agentica.conversate()` function, [#Multi Agent Orchestration](/docs/concepts/function-calling/#orchestration-strategy) to the internal sub agents would be processed including function callings and executions. When the orchestration has been completed, the `Agentica.conversate()` function will return the list of newly created prompts.

When you want to archive the conversation state of current agent, store the returned prompots to your database serializing them from `AgenticaPrompt` type to JSON format's `IAgenticaPromptJson` type. And then restore the conversation state by assigning the prompt histories to the `IAgenticaProps.histories` property when creating a new `Agentica` instance.

For reference, loading and saving the prompt histories, it is possible to change its content or remove some of them for management purpose. For example, you can summarize some or all prompts to a single prompt to reduce the LLM token costs.

If you're not considering the summarization, but just hope to remove some of the prompt histories, you can remove `AgenticaPrompt.Select` and `AgenticaPrompt.Execute` types. In that case, remained `AgenticaPrompt.Describe` type will keep the context.




## Text Prompt
<Tabs items={[
  <code>src/main.ts</code>,
  <code>AgenticaPrompt</code>,
  <code>AgenticaOperation</code>,
]}>
  <Tabs.Tab>
```typescript filename="src/main.ts" showLineNumbers {17}
import { Agentica, AgenticaPrompt } from "@agentica/core";
import OpenAI from "openai";

const agent = new Agentica({
  model: "chatgpt",
  vendor: {
    api: new OpenAI({ apiKey: "*****" }),
    model: "gpt-4o-mini",
  },
  controllers: [...],
});

const prompts: AgenticaPrompt[] = await agent.conversate(
  "I wanna buy MacBook Pro",
);
for (const p of prompts)
  if (p.type === "text") console.log(p.role, p.text);
```
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaPromptSnippet />
  </Tabs.Tab>
</Tabs>

Text conversation history.

`AgenticaPrompt.Text` is a type of history that represents the text conversation between the user and the agent. And it is separated by the role of the speaker, which is either `user` or `agent`.




## Select Prompt
<Tabs items={[
  <code>src/main.ts</code>,
  <code>AgenticaPrompt</code>,
  <code>AgenticaOperationSelection</code>,
  <code>AgenticaOperation</code>,
]}>
  <Tabs.Tab>
```typescript filename="src/main.ts" showLineNumbers {17}
import { Agentica, AgenticaPrompt } from "@agentica/core";
import OpenAI from "openai";

const agent = new Agentica({
  model: "chatgpt",
  vendor: {
    api: new OpenAI({ apiKey: "*****" }),
    model: "gpt-4o-mini",
  },
  controllers: [...],
});

const prompts: AgenticaPrompt[] = await agent.conversate(
  "I wanna buy MacBook Pro",
);
for (const p of prompts)
  if (p.type === "select")
    for (const selection of p.selections) 
      console.log(
        "select",
        selection.operation.name,
        selection.reason,
      );
```
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaPromptSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSelectionSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSnippet />
  </Tabs.Tab>
</Tabs>

Selection history of the candidate functions.

`AgenticaPrompt.Select` is a type of history that represents the selection of the candidate functions to call with detailed reasons.

For reference, as the selection is sometimes done by grouping several functions together, if `AgenticaPrompt.Select.selections` has more than one element, it means that the selection to multiple functions are made by just one conversation step.

Also, if you want to reduce the number of prompt histories to reduce the LLM token costs, this `AgenticaPrompt.Select` type can be the elimination target with `AgenticaPrompt.Execute` type. Instead, you have to keep the `AgenticaPrompt.Describe` type to remain the context.



## Execute Prompt
<Tabs items={[
  <code>src/main.ts</code>,
  <code>AgenticaPrompt</code>,
  <code>AgenticaOperationSelection</code>,
  <code>AgenticaOperation</code>,
]}>
  <Tabs.Tab>
```typescript filename="src/main.ts" showLineNumbers
import { Agentica, AgenticaPrompt } from "@agentica/core";
import OpenAI from "openai";

const agent = new Agentica({
  model: "chatgpt",
  vendor: {
    api: new OpenAI({ apiKey: "*****" }),
    model: "gpt-4o-mini",
  },
  controllers: [...],
});
const prompts: AgenticaPrompt[] = await agent.conversate(
  "I wanna buy MacBook Pro",
);
for (const p of prompts)
  if (p.type === "execute")
    console.log(
      "execute",
      p.operation.name,
      p.arguments,
      p.value,
    );
```
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaPromptSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSelectionSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSnippet />
  </Tabs.Tab>
</Tabs>

Execution history of the function call.

`AgenticaPrompt.Execute` is a type of history that represents the execution of the function call with its arguments composed by LLM function calling, and return value by the execution. If a target function comes from an HTTP controller, the return value also contains the status and headers of the HTTP response.

For reference, this `AgenticaPrompt.Execute` often becomes the elimination target with `AgenticaPrompt.Select` when you want to reduce the number of prompt histories to reduce the LLM token costs. It is okay to eliminating `AgenticaPrompt.Execute` and `AgenticaPrompt.Select` types from the prompt histories, but you have to keep the `AgenticaPrompt.Describe` type to remain the context.




## Describe Prompt
<Tabs items={[
  <code>src/main.ts</code>,
  <code>AgenticaPrompt</code>,
  <code>AgenticaOperationSelection</code>,
  <code>AgenticaOperation</code>,
]}>
  <Tabs.Tab>
```typescript filename="src/main.ts" showLineNumbers
import { Agentica, AgenticaPrompt } from "@agentica/core";
import OpenAI from "openai";

const agent = new Agentica({
  model: "chatgpt",
  vendor: {
    api: new OpenAI({ apiKey: "*****" }),
    model: "gpt-4o-mini",
  },
  controllers: [...],
});
const prompts: AgenticaPrompt[] = await agent.conversate(
  "I wanna buy MacBook Pro",
);
for (const p of prompts)
  if (p.type === "describe")
    console.log(
      "describe",
      p.executes.map((e) => e.operation.name),
      p.text,
    );
```
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaPromptSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSelectionSnippet />
  </Tabs.Tab>
  <Tabs.Tab>
    <AgenticaOperationSnippet />
  </Tabs.Tab>
</Tabs>

Description history of function calling results.

`AgenticaPrompt.Describe` is a type of history that represents the description of the function calling results. It contains the list of target function execution prompts, and the text description about the results.

If you want to reduce the LLM token costs, make `AgenticaPrompt.Describe.text` to be shorter by configuring system prompt of the describe agent, or summarize `AgenticaPrompt.Text` and `AgenticaPrompt.Describe` types to a single prompt after eliminating the `AgenticaPrompt.Select` and `AgenticaPrompt.Execute` types.